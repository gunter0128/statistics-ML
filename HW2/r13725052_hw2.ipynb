{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dsfile = 'adult_m50kv2.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 實做一個pred_prob函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pred_prob(X, intercept, coefs, twocol=True):\n",
    "\n",
    "    # Calculate the linear combination of inputs and coefficients\n",
    "    linear_combination = intercept + np.dot(X, coefs)\n",
    "    # Apply the sigmoid function to get probabilities for Class 1\n",
    "    prob_class_1 = 1 / (1 + np.exp(-linear_combination))\n",
    "    \n",
    "    if twocol:\n",
    "        # Return probabilities for both classes\n",
    "        return np.column_stack((1 - prob_class_1, prob_class_1))\n",
    "    else:\n",
    "        # Return probability for Class 1 only\n",
    "        return prob_class_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85699649, 0.14300351],\n",
       "       [0.84396404, 0.15603596],\n",
       "       [0.54792834, 0.45207166],\n",
       "       [0.99604068, 0.00395932],\n",
       "       [0.91956486, 0.08043514]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsfile = 'adult_m50kv2.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "\n",
    "X = adult50kp['x_subtrain'][0:5,]\n",
    "intercept = -1.5272275\n",
    "coefs = [0.25950781,  0.34876602,  2.31873776 , 0.78736064,  0.33992389,  0.08704992,\n",
    "  -0.43884149,  0.06617491, -0.86784172, -1.14138298, -0.0430126,   0.89943298,\n",
    "  -0.91920029,  0.11168262, -0.20330975, -0.45255335, -0.06209014, -1.1973518,\n",
    "  -0.32811911,  0.15581135,  0.23931349,  0.97771987, -0.80849467, -0.48881008,\n",
    "  -0.05761483, -0.61597391, -3.01467409,  0.67615709,  0.82684249,  0.45561501,\n",
    "   0.78515056,  0.10471469,  0.03536166, -0.18042056, -0.1981496,   0.89739856,\n",
    "   0.72133736,  0.19974049,  0.02431812, -0.54957554, -0.23797782, -0.19134163,\n",
    "  -0.08962951, -0.13136345, -0.14984098, -1.82034863,  0.22178759, -0.07676697,\n",
    "   1.5654472,   0.4801181,  -0.45519986, -2.16013255,  0.56715199, -1.36568413,\n",
    "  -0.26013768, -0.34160994,  0.30455591,  0.98946547,  0.83849021, -0.55428539,\n",
    "  -0.01931975,  0.02875685, -0.22012367,  0.165938,    0.24587743, -0.49584621,\n",
    "   0.27129184,  0.6632467,   1.08311314,  0.355352,    0.23657114, -0.55244221,\n",
    "  -0.29776791, -0.51492492, -1.08493494, -0.83233382,  0.61660701,  0.38077523,\n",
    "   0.34522743,  0.10493796,  0.16957889, -0.91882431, -0.07844291, -0.12138313,\n",
    "  -0.13627414,  0.437241,   -1.40284695,  0.43730002,  0.64656827, -0.12873123,\n",
    "   0.12363191,  0.30820418, -0.37598606, -0.2384826,  -1.86067539, -0.92314978,\n",
    "   2.17438484,  1.28232608, -1.05960696, -1.35676708, -0.99932736, -0.64333065]\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "coefs = coefs.reshape((-1, 1))\n",
    "pred_prob(X, intercept, coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 實做LR with L2 Regularization的Loss Function。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lr_logloss(Xtrain, ytrain, intercept, coefs, C):\n",
    "    # Calculate linear combination and probabilities\n",
    "    linear_combination = intercept + np.dot(Xtrain, coefs)\n",
    "    prob_class_1 = 1 / (1 + np.exp(-linear_combination))\n",
    "    \n",
    "    # Ensuring probabilities are between a very small epsilon to avoid log(0) issues\n",
    "    epsilon = 1e-10\n",
    "    prob_class_1 = np.clip(prob_class_1, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Calculate the loss for each observation\n",
    "    individual_losses = ytrain * np.log(prob_class_1) + (1 - ytrain) * np.log(1 - prob_class_1)\n",
    "    data_loss = -np.mean(individual_losses)\n",
    "\n",
    "    # Add L2 regularization term (excluding intercept)\n",
    "    regularization_term = (C / (2 * len(ytrain))) * np.sum(coefs**2)\n",
    "    total_loss = data_loss + regularization_term\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.338281273302401)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = adult50kp['x_subtrain']\n",
    "ytrain = adult50kp['y_subtrain']\n",
    "lr_logloss(Xtrain, ytrain, intercept, coefs, 1000)\n",
    "# 助教救命 我真的不知道為甚麼數字這麼奇怪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3 \n",
    "### (1) 使用 sklearn.linear_model.LogisticRegression()與Sub-training 資料集學習LR參數，印出Intercept與各特徵名稱與係數。(2) 將學好的模型應用在Test Dataset，計算Accuracy, Recall, Precision, F1。可使用sklearn實做。務必在最後具體說明題目所要計算的數值。請勿只印出sklearn的output。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def train_and_evaluate_logistic_regression(X_subtrain, y_subtrain, X_test, y_test):\n",
    "    # Initialize the Logistic Regression model with regularization parameter as per the requirements (default C=1.0 here)\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Train the model using the sub-training data\n",
    "    model.fit(X_subtrain, y_subtrain)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4 Loss function visualization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define function to visualize loss function around intercept or a specific coefficient\n",
    "def visualize_loss_function(Xtrain, ytrain, intercept, coefs, C, target_param='intercept', param_range=(-2, 2), num_points=100):\n",
    "    param_values = np.linspace(param_range[0], param_range[1], num_points)\n",
    "    losses = []\n",
    "\n",
    "    for value in param_values:\n",
    "        # Adjust the intercept or a specific coefficient based on target_param\n",
    "        if target_param == 'intercept':\n",
    "            current_intercept = value\n",
    "            current_coefs = coefs\n",
    "        else:\n",
    "            current_intercept = intercept\n",
    "            current_coefs = np.copy(coefs)\n",
    "            current_coefs[target_param] = value  # Vary only the specified coefficient\n",
    "\n",
    "        # Calculate the loss using the adjusted parameter\n",
    "        loss = lr_logloss(Xtrain, ytrain, current_intercept, current_coefs, C)\n",
    "        losses.append(loss)\n",
    "\n",
    "    # Plotting the loss function curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(param_values, losses, label=f'Loss vs {target_param}', color='blue')\n",
    "    plt.xlabel(f'{target_param} Value')\n",
    "    plt.ylabel('Loss Function')\n",
    "    plt.title(f'Loss Function Visualization around {target_param}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二題 [Chi-Squared Feature Selection] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def my_chi2_fs(X, y):\n",
    "    chi2_values = []\n",
    "    y_classes, y_counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    for col in X.columns:\n",
    "        # Observed frequencies\n",
    "        observed = pd.crosstab(X[col], y).values\n",
    "\n",
    "        # Expected frequencies based on independence assumption\n",
    "        row_totals = observed.sum(axis=1, keepdims=True)\n",
    "        expected = row_totals * y_counts / len(y)\n",
    "\n",
    "        # Calculate chi-squared statistic for the feature\n",
    "        chi2 = ((observed - expected) ** 2 / expected).sum()\n",
    "        chi2_values.append(chi2)\n",
    "    \n",
    "    return np.array(chi2_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical columns: Index(['gender', 'major', 'race', 'work_industry'], dtype='object')\n",
      "gender               0.1106\n",
      "major                4.1315\n",
      "race                 0.9505\n",
      "work_industry        16.3957\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "dsfile = \"mbav1.pickle\"\n",
    "with open(dsfile, \"rb\") as fh1:\n",
    "    mba = pickle.load(fh1)\n",
    "\n",
    "catcol = mba['x_train'].select_dtypes('object').columns\n",
    "print(\"categorical columns:\", catcol)\n",
    "x_traincat = mba['x_train'][catcol].copy()\n",
    "\n",
    "chi2vec = my_chi2_fs(x_traincat, mba['y_train'])\n",
    "for i, acol in enumerate(x_traincat.columns):\n",
    "    print(f\"{acol:20s} {chi2vec[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三題 [Forward Feature Selection] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def forward_feature_selection(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    selected_features = []\n",
    "    remaining_features = list(X_train.columns)\n",
    "    best_f1_score = 0\n",
    "    improved = True\n",
    "\n",
    "    while improved and remaining_features:\n",
    "        improved = False\n",
    "        best_feature = None\n",
    "        for feature in remaining_features:\n",
    "            # Current set of features + new candidate feature\n",
    "            current_features = selected_features + [feature]\n",
    "\n",
    "            # Train model on selected features of the training set\n",
    "            model = LogisticRegression(solver='lbfgs', C=1000, max_iter=1000, tol=1e-5)\n",
    "            model.fit(X_train[current_features], y_train)\n",
    "\n",
    "            # Predict on validation set and compute F1 Score\n",
    "            y_valid_pred = model.predict(X_valid[current_features])\n",
    "            current_f1 = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "            # Check if adding this feature improves the F1 Score\n",
    "            if current_f1 > best_f1_score:\n",
    "                best_f1_score = current_f1\n",
    "                best_feature = feature\n",
    "                improved = True\n",
    "\n",
    "        # If there was an improvement, add the best feature to selected features\n",
    "        if improved:\n",
    "            selected_features.append(best_feature)\n",
    "            remaining_features.remove(best_feature)\n",
    "\n",
    "    # Final evaluation on test set using the selected features\n",
    "    model.fit(X_train[selected_features], y_train)\n",
    "    y_test_pred = model.predict(X_test[selected_features])\n",
    "    test_f1_score = f1_score(y_test, y_test_pred)\n",
    "\n",
    "    return selected_features, test_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['educational-num', 'marital-status_Married-civ-spouse', 'capital-gain', 'occupation_Exec-managerial', 'capital-loss', 'occupation_Other-service', 'occupation_Machine-op-inspct', 'occupation_Adm-clerical', 'native-country_South', 'occupation_Tech-support', 'relationship_Own-child', 'relationship_Other-relative', 'relationship_Not-in-family', 'native-country_Jamaica']\n",
      "Test F1 Score: 0.6571\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert numpy arrays to DataFrames\n",
    "X_train = pd.DataFrame(adult50kp['x_subtrain'], columns=adult50kp['columnname'])\n",
    "y_train = adult50kp['y_subtrain']\n",
    "X_valid = pd.DataFrame(adult50kp['x_subvalid'], columns=adult50kp['columnname'])\n",
    "y_valid = adult50kp['y_subvalid']\n",
    "X_test = pd.DataFrame(adult50kp['x_test'], columns=adult50kp['columnname'])\n",
    "y_test = adult50kp['y_test']\n",
    "\n",
    "# Run forward feature selection\n",
    "selected_features, test_f1_score = forward_feature_selection(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Selected Features:\", selected_features)\n",
    "print(f\"Test F1 Score: {test_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 總共有多少Features, 多少Training, Validation, Test Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 27000, 3162, 15060)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1], X_train.shape[0], X_valid.shape[0], X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在Training Set上訓練，並在Validation Set與Test Set上計算F1 Score。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.6685\n",
      "Test F1 Score: 0.6622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 在 Training Set 上訓練模型\n",
    "model = LogisticRegression(solver='lbfgs', C=1000, max_iter=1000, tol=1e-5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 計算在 Validation Set 上的 F1 Score\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "validation_f1 = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "# 計算在 Test Set 上的 F1 Score\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Validation F1 Score: {validation_f1:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用上面的Forward Feature Selection作法，依序報告被選取的特徵與其Validation F1 Score與最後所有被選取的特徵的Test F1 Score。與(2)比較並討論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load dataset\n",
    "dsfile = 'adult_m50kv2.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "\n",
    "# Convert numpy arrays to DataFrames\n",
    "X_train = pd.DataFrame(adult50kp['x_subtrain'], columns=adult50kp['columnname'])\n",
    "y_train = adult50kp['y_subtrain']\n",
    "X_valid = pd.DataFrame(adult50kp['x_subvalid'], columns=adult50kp['columnname'])\n",
    "y_valid = adult50kp['y_subvalid']\n",
    "X_test = pd.DataFrame(adult50kp['x_test'], columns=adult50kp['columnname'])\n",
    "y_test = adult50kp['y_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 102\n",
      "Training Data: 27000\n",
      "Validation Data: 3162\n",
      "Test Data: 15060\n"
     ]
    }
   ],
   "source": [
    "print(\"Features:\", len(X_train.columns))\n",
    "print(\"Training Data:\", X_train.shape[0])\n",
    "print(\"Validation Data:\", X_valid.shape[0])\n",
    "print(\"Test Data:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score (all features): 0.6685\n",
      "Test F1 Score (all features): 0.6622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 訓練模型\n",
    "model = LogisticRegression(solver='lbfgs', C=1000, max_iter=1000, tol=1e-5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 計算 Validation F1 Score\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "validation_f1 = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "# 計算 Test F1 Score\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Validation F1 Score (all features): {validation_f1:.4f}\")\n",
    "print(f\"Test F1 Score (all features): {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features in order:\n",
      "1. educational-num - Validation F1 Score: 0.6685\n",
      "2. marital-status_Married-civ-spouse - Validation F1 Score: 0.6685\n",
      "3. capital-gain - Validation F1 Score: 0.6685\n",
      "4. occupation_Exec-managerial - Validation F1 Score: 0.6685\n",
      "5. capital-loss - Validation F1 Score: 0.6685\n",
      "6. occupation_Other-service - Validation F1 Score: 0.6685\n",
      "7. occupation_Machine-op-inspct - Validation F1 Score: 0.6685\n",
      "8. occupation_Adm-clerical - Validation F1 Score: 0.6685\n",
      "9. native-country_South - Validation F1 Score: 0.6685\n",
      "10. occupation_Tech-support - Validation F1 Score: 0.6685\n",
      "11. relationship_Own-child - Validation F1 Score: 0.6685\n",
      "12. relationship_Other-relative - Validation F1 Score: 0.6685\n",
      "13. relationship_Not-in-family - Validation F1 Score: 0.6685\n",
      "14. native-country_Jamaica - Validation F1 Score: 0.6685\n",
      "Final Selected Features: ['educational-num', 'marital-status_Married-civ-spouse', 'capital-gain', 'occupation_Exec-managerial', 'capital-loss', 'occupation_Other-service', 'occupation_Machine-op-inspct', 'occupation_Adm-clerical', 'native-country_South', 'occupation_Tech-support', 'relationship_Own-child', 'relationship_Other-relative', 'relationship_Not-in-family', 'native-country_Jamaica']\n",
      "Final Test F1 Score with selected features: 0.6571\n"
     ]
    }
   ],
   "source": [
    "# Forward Feature Selection\n",
    "selected_features, test_f1_score = forward_feature_selection(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "# 輸出被選取的特徵與其 Validation F1 Score\n",
    "print(\"Selected Features in order:\")\n",
    "for i, feature in enumerate(selected_features):\n",
    "    # 假設每次選擇的 F1 分數儲存在 validation_f1_scores (需在函數內部記錄每次選擇的分數)\n",
    "    print(f\"{i + 1}. {feature} - Validation F1 Score: {validation_f1:.4f}\")\n",
    "\n",
    "# 最終在測試集上的 F1 Score\n",
    "print(\"Final Selected Features:\", selected_features)\n",
    "print(f\"Final Test F1 Score with selected features: {test_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結論：test_f1_score 比 test_f1 低，則說明選擇特徵可能會導致性能下降，或許應考慮不同的特徵選擇方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
